# ğŸ”· End-to-End Azure Data Engineering Project

This project demonstrates how to build a **modern data pipeline** using Azure services â€” from data ingestion to transformation, and finally to reporting. It was developed as part of my learning journey as a Data Science student exploring cloud-based data engineering.

---

## ğŸš€ Project Overview

This pipeline is built using:

- **Azure Data Factory** for data ingestion and orchestration
- **Azure Data Lake Storage Gen2** for storing raw and transformed data
- **Azure Databricks (Apache Spark)** for scalable data transformation using PySpark
- **Azure Synapse Analytics (Serverless SQL Pools)** for querying data directly from the Data Lake via external tables
- **Power BI** for creating final visual reports

---

## ğŸ§  What I Learned

- How to build and orchestrate **ETL/ELT workflows** using ADF and Databricks
- Designing **layered architecture** (raw â†’ curated â†’ consumption) in a Data Lake
- Using **external tables** in Synapse to query Parquet/CSV files directly â€” reducing cost and complexity
- Leveraging **Serverless SQL** for efficient, pay-per-query analytics
- Creating **interactive dashboards** using Power BI
- Best practices for **partitioning**, **data pipeline optimization**, and **interview prep topics** for Azure Data Engineering roles

---

## ğŸ—ï¸ Architecture Overview

```plaintext
[External Source]
       â†“
Azure Data Factory (Ingestion)
       â†“
Azure Data Lake (Raw Zone)
       â†“
Azure Databricks (Transform with PySpark)
       â†“
Azure Data Lake (Curated Zone)
       â†“
Azure Synapse (Serverless SQL + External Tables)
       â†“
Power BI (Reporting & Dashboarding)
